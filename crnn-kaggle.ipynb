{"cells":[{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.062763Z","iopub.status.busy":"2023-11-26T10:11:44.061706Z","iopub.status.idle":"2023-11-26T10:11:44.068281Z","shell.execute_reply":"2023-11-26T10:11:44.067111Z","shell.execute_reply.started":"2023-11-26T10:11:44.062715Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{},"source":["## DATASET: Loading and Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["The dataset comes in different formats. These two classes are designed to read annotation files where each line points to an image file and its corresponding label. It supports different annotation formats, like in MJSynth and ICDAR datasets."]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.070653Z","iopub.status.busy":"2023-11-26T10:11:44.070307Z","iopub.status.idle":"2023-11-26T10:11:44.078630Z","shell.execute_reply":"2023-11-26T10:11:44.077717Z","shell.execute_reply.started":"2023-11-26T10:11:44.070621Z"},"trusted":true},"outputs":[],"source":["img_shape1=(32, 128, 3)  # 32 x 128 pixels, RGC images\n","# It is crucial that all input to the CRNN has consistent shape and size"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.080418Z","iopub.status.busy":"2023-11-26T10:11:44.080033Z","iopub.status.idle":"2023-11-26T10:11:44.106123Z","shell.execute_reply":"2023-11-26T10:11:44.105072Z","shell.execute_reply.started":"2023-11-26T10:11:44.080373Z"},"trusted":true},"outputs":[],"source":["try:\n","    AUTOTUNE = tf.data.AUTOTUNE\n","except AttributeError:\n","    # tf < 2.4.0\n","    AUTOTUNE = tf.data.experimental.AUTOTUNE # automatically tune various parameters dynamically at runtime\n","\n","\n","# This reads the text file where each line represents an image ??\n","class Dataset(tf.data.TextLineDataset):\n","    def __init__(self, filename, **kwargs):\n","        self.dirname = os.path.dirname(filename)\n","        super().__init__(filename, **kwargs)\n","\n","    def parse_func(self, line):\n","        raise NotImplementedError\n","        \n","        \n","    # Reads a line from the file, splits it into image path and label\n","    def parse_line(self, line):\n","        line = tf.strings.strip(line)\n","        img_relative_path, label = self.parse_func(line)\n","        img_path = tf.strings.join([self.dirname, os.sep, img_relative_path])\n","        #img_path = tf.strings.join(['/kaggle/input/mjsynth/mnt/ramdisk/max/90kDICT32px', os.sep, img_relative_path])  ## CHANGED\n","        return img_path, label\n","\n","\"\"\"\n","class SimpleDataset(Dataset):\n","    def parse_func(self, line):\n","        splited_line = tf.strings.split(line)\n","        img_relative_path, label = splited_line[0], splited_line[1]\n","        return img_relative_path, label\n","\"\"\"\n","\n","class MJSynthDataset(Dataset):\n","    def parse_func(self, line):\n","        splited_line = tf.strings.split(line)\n","        img_relative_path = splited_line[0]\n","        label = tf.strings.split(img_relative_path, sep=\"_\")[1]\n","        return img_relative_path, label\n","\n","\n","class ICDARDataset(Dataset):\n","    def parse_func(self, line):\n","        splited_line = tf.strings.split(line, sep=\",\")\n","        img_relative_path, label = splited_line[0], splited_line[1]\n","        label = tf.strings.strip(label)\n","        label = tf.strings.regex_replace(label, r'\"', \"\")\n","        return img_relative_path, label\n","\n","\n","class DatasetBuilder:\n","    def __init__(\n","        self,\n","        table_path,\n","        img_shape=img_shape1,\n","        max_img_width=300,\n","        ignore_case=False,\n","    ):\n","       # Static HashTable to lookup the labels \n","        # map unknown label to 0\n","        self.table = tf.lookup.StaticHashTable(\n","            tf.lookup.TextFileInitializer(\n","                table_path,\n","                tf.string,\n","                tf.lookup.TextFileIndex.WHOLE_LINE,  # either a whole line is used as a key\n","                tf.int64,\n","                tf.lookup.TextFileIndex.LINE_NUMBER, # or a line number\n","            ),\n","            0,\n","        )\n","        self.img_shape = img_shape\n","        self.ignore_case = ignore_case\n","        if img_shape[1] is None:\n","            self.max_img_width = max_img_width\n","            self.preserve_aspect_ratio = True\n","        else:\n","            self.preserve_aspect_ratio = False\n","\n","    # the number of unique classes/labels.        \n","    @property\n","    def num_classes(self):\n","        return self.table.size()\n","    \n","    \n","    #  Determines the format of the annotation file according to the appropriate dataset\n","    def _parse_annotation(self, path):\n","        with open(path) as f:\n","            line = f.readline().strip()\n","        if re.fullmatch(r\".*/*\\d+_.+_(\\d+)\\.\\w+ \\1\", line):\n","            return MJSynthDataset(path)\n","        elif re.fullmatch(r'.*/*word_\\d\\.\\w+, \".+\"', line):\n","            return ICDARDataset(path)\n","        elif re.fullmatch(r\".+\\.\\w+ .+\", line):\n","            return SimpleDataset(path)\n","        else:\n","            raise ValueError(\"Unsupported annotation format\")\n","    # Concatenate \n","    def _concatenate_ds(self, ann_paths):\n","        datasets = [self._parse_annotation(path) for path in ann_paths]\n","        concatenated_ds = datasets[0].map(datasets[0].parse_line)\n","        for ds in datasets[1:]:\n","            ds = ds.map(ds.parse_line)\n","            concatenated_ds = concatenated_ds.concatenate(ds)\n","        return concatenated_ds\n","\n","    def _decode_img(self, filename, label):\n","        img = tf.io.read_file(filename)\n","        img = tf.io.decode_jpeg(img, channels=self.img_shape[-1])\n","        \n","        img = tf.image.resize_with_pad(img, self.img_shape[0], self.img_shape[1]) / 255.0 # normalized image\n","        \n","        return img, label\n","\n","    # exclude images wider than a maximum width\n","    def _filter_img(self, img, label):\n","        img_shape = tf.shape(img)\n","        return img_shape[1] < self.max_img_width\n","\n","    #  Converts text labels into numerical tokens using a lookup table\n","    def _tokenize(self, imgs, labels):\n","        chars = tf.strings.unicode_split(labels, \"UTF-8\")\n","        tokens = tf.ragged.map_flat_values(self.table.lookup, chars)\n","        # TODO(hym) Waiting for official support to use RaggedTensor in keras\n","        tokens = tokens.to_sparse()\n","        return imgs, tokens\n","\n","    # Constructs the dataset by concatenating, decoding, filtering, batching, and tokenizing the data. I\n","    def __call__(self, ann_paths, batch_size, is_training):\n","        ds = self._concatenate_ds(ann_paths)\n","        if self.ignore_case:\n","            ds = ds.map(lambda x, y: (x, tf.strings.lower(y)))\n","        if is_training:\n","            ds = ds.shuffle(buffer_size=10000)\n","        ds = ds.map(self._decode_img, AUTOTUNE)\n","        if self.preserve_aspect_ratio and batch_size != 1:\n","            ds = ds.filter(self._filter_img)\n","            ds = ds.padded_batch(batch_size, drop_remainder=is_training)\n","        else:\n","            ds = ds.batch(batch_size, drop_remainder=is_training)\n","        ds = ds.map(self._tokenize, AUTOTUNE)\n","        ds = ds.prefetch(AUTOTUNE)\n","        return ds"]},{"cell_type":"markdown","metadata":{},"source":["## Decoders:\n","- GreedyDecoder\n","- BeamSearchDecoder"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.108556Z","iopub.status.busy":"2023-11-26T10:11:44.108250Z","iopub.status.idle":"2023-11-26T10:11:44.124642Z","shell.execute_reply":"2023-11-26T10:11:44.123675Z","shell.execute_reply.started":"2023-11-26T10:11:44.108532Z"},"trusted":true},"outputs":[],"source":["\n","class CTCDecoder(keras.layers.Layer): # Base Class\n","    def __init__(self, table_path, **kwargs):\n","        super().__init__(**kwargs)\n","        self.table = tf.lookup.StaticHashTable(\n","            tf.lookup.TextFileInitializer(\n","                table_path,\n","                tf.int64,\n","                tf.lookup.TextFileIndex.LINE_NUMBER,\n","                tf.string,\n","                tf.lookup.TextFileIndex.WHOLE_LINE,\n","            ),\n","            \"\",\n","        )\n","\n","    def detokenize(self, x):\n","        x = tf.RaggedTensor.from_sparse(x)\n","        x = tf.ragged.map_flat_values(self.table.lookup, x)\n","        strings = tf.strings.reduce_join(x, axis=1)\n","        return strings\n","\n","\n","class CTCGreedyDecoder(CTCDecoder):\n","    def __init__(self, table_path, merge_repeated=True, **kwargs):\n","        super().__init__(table_path, **kwargs)\n","        self.merge_repeated = merge_repeated\n","\n","    def call(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        sequence_length = tf.fill([input_shape[0]], input_shape[1])\n","        decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n","            tf.transpose(inputs, perm=[1, 0, 2]),\n","            sequence_length,\n","            self.merge_repeated,\n","        )\n","        strings = self.detokenize(decoded[0])\n","        labels = tf.cast(decoded[0], tf.int32)\n","        loss = tf.nn.ctc_loss( \n","            labels=labels,\n","            logits=inputs,\n","            label_length=None,\n","            logit_length=sequence_length,\n","            logits_time_major=False,\n","            blank_index=-1,\n","        )\n","        probability = tf.math.exp(-loss)  # calculates a probablity score based on the CTC loss\n","        return strings, probability\n","\n","    \n","# Considers multiple paths for decoding => can get better accuracy but slower\n","class CTCBeamSearchDecoder(CTCDecoder):\n","    def __init__(self, table_path, beam_width=100, top_paths=1, **kwargs):\n","        super().__init__(table_path, **kwargs)\n","        self.beam_width = beam_width\n","        self.top_paths = top_paths\n","\n","    def call(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        decoded, log_probability = tf.nn.ctc_beam_search_decoder(\n","            tf.transpose(inputs, perm=[1, 0, 2]),\n","            tf.fill([input_shape[0]], input_shape[1]),\n","            self.beam_width,\n","            self.top_paths,\n","        )\n","        strings = []\n","        for i in range(self.top_paths):\n","            strings.append(self.detokenize(decoded[i]))\n","        strings = tf.concat(strings, 1)\n","        probability = tf.math.exp(log_probability)  # calculates a probablity score based \n","        return strings, probability                 # on the log probabilities returned by the beam search decoder.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Losses: CTC loss\n","Connectionist temporal classification"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.168097Z","iopub.status.busy":"2023-11-26T10:11:44.167376Z","iopub.status.idle":"2023-11-26T10:11:44.178360Z","shell.execute_reply":"2023-11-26T10:11:44.177470Z","shell.execute_reply.started":"2023-11-26T10:11:44.168061Z"},"trusted":true},"outputs":[],"source":["# import tensorflow as tf\n","# from tensorflow import keras\n","\n","\n","class CTCLoss(keras.losses.Loss):\n","    \"\"\"A class that wraps the function of tf.nn.ctc_loss.\n","\n","    Attributes:\n","        logits_time_major: If False (default) , shape is [batch, time, logits],\n","            If True, logits is shaped [time, batch, logits].\n","        blank_index: Set the class index to use for the blank label. default is\n","            -1 (num_classes - 1).\n","    \"\"\"\n","\n","    def __init__(\n","        self, logits_time_major=False, blank_index=-1, name=\"ctc_loss\"\n","    ):\n","        super().__init__(name=name)\n","        self.logits_time_major = logits_time_major\n","        self.blank_index = blank_index\n","\n","    def call(self, y_true, y_pred):\n","        \"\"\"Computes CTC (Connectionist Temporal Classification) loss. work on\n","        CPU, because y_true is a SparseTensor.\n","        \"\"\"\n","        y_true = tf.cast(y_true, tf.int32)\n","        y_pred_shape = tf.shape(y_pred)\n","        logit_length = tf.fill([y_pred_shape[0]], y_pred_shape[1])\n","        loss = tf.nn.ctc_loss(\n","            labels=y_true,\n","            logits=y_pred,\n","            label_length=None,\n","            logit_length=logit_length, # pred length\n","            logits_time_major=self.logits_time_major,\n","            blank_index=self.blank_index,\n","        )\n","        return tf.math.reduce_mean(loss)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Metrics\n","- Sequence Accuracy: exact accuracy, measures the percentage of correct sequences.\n","- Edit Distance: not exact prediction, measures the average number of insertions, deletions, or substitutions required to change predicted sequences into the correct ones.\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.179960Z","iopub.status.busy":"2023-11-26T10:11:44.179612Z","iopub.status.idle":"2023-11-26T10:11:44.198660Z","shell.execute_reply":"2023-11-26T10:11:44.197748Z","shell.execute_reply.started":"2023-11-26T10:11:44.179922Z"},"trusted":true},"outputs":[],"source":["# import tensorflow as tf\n","# from tensorflow import keras\n","\n","\n","class SequenceAccuracy(keras.metrics.Metric):\n","    def __init__(self, name=\"sequence_accuracy\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n","        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        def sparse2dense(tensor, shape):\n","            tensor = tf.sparse.reset_shape(tensor, shape)\n","            tensor = tf.sparse.to_dense(tensor, default_value=-1)\n","            tensor = tf.cast(tensor, tf.float32)\n","            return tensor\n","\n","        y_true_shape = tf.shape(y_true)\n","        batch_size = y_true_shape[0]\n","        y_pred_shape = tf.shape(y_pred)\n","        max_width = tf.math.maximum(y_true_shape[1], y_pred_shape[1])\n","        logit_length = tf.fill([batch_size], y_pred_shape[1])\n","        decoded, _ = tf.nn.ctc_greedy_decoder(\n","            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n","            sequence_length=logit_length,\n","        )\n","        y_true = sparse2dense(y_true, [batch_size, max_width])\n","        y_pred = sparse2dense(decoded[0], [batch_size, max_width])\n","        num_errors = tf.math.reduce_any(\n","            tf.math.not_equal(y_true, y_pred), axis=1\n","        )\n","        num_errors = tf.cast(num_errors, tf.float32)\n","        num_errors = tf.math.reduce_sum(num_errors)\n","        batch_size = tf.cast(batch_size, tf.float32)\n","        self.total.assign_add(batch_size)\n","        self.count.assign_add(batch_size - num_errors)\n","\n","    def result(self):\n","        return self.count / self.total\n","\n","    def reset_states(self):\n","        self.count.assign(0)\n","        self.total.assign(0)\n","\n","\n","class EditDistance(keras.metrics.Metric):\n","    def __init__(self, name=\"edit_distance\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n","        self.sum_distance = self.add_weight(\n","            name=\"sum_distance\", initializer=\"zeros\"\n","        )\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred_shape = tf.shape(y_pred)\n","        batch_size = y_pred_shape[0]\n","        logit_length = tf.fill([batch_size], y_pred_shape[1])\n","        decoded, _ = tf.nn.ctc_greedy_decoder(\n","            inputs=tf.transpose(y_pred, perm=[1, 0, 2]),\n","            sequence_length=logit_length,\n","        )\n","        sum_distance = tf.math.reduce_sum(tf.edit_distance(decoded[0], y_true))\n","        batch_size = tf.cast(batch_size, tf.float32)\n","        self.sum_distance.assign_add(sum_distance)\n","        self.total.assign_add(batch_size)\n","\n","    def result(self):\n","        return self.sum_distance / self.total\n","\n","    def reset_states(self):\n","        self.sum_distance.assign(0)\n","        self.total.assign(0)"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","**Convolutional Recurrent Neural Network:** combining VGG-style convolutional layers for feature extraction and **bidirectional LSTM** layers for **sequence modeling**.The pre and post processing steps are left optional, for a specified number of output classes.\n",">VGG stands for Visual Graphics Group, make sure that the architecture is uniform"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.201445Z","iopub.status.busy":"2023-11-26T10:11:44.201139Z","iopub.status.idle":"2023-11-26T10:11:44.218916Z","shell.execute_reply":"2023-11-26T10:11:44.217773Z","shell.execute_reply.started":"2023-11-26T10:11:44.201419Z"},"trusted":true},"outputs":[],"source":["# from tensorflow import keras\n","# from tensorflow.keras import layers\n","\n","\n","def vgg_style(x):\n","    \"\"\"\n","    The original feature extraction structure from CRNN paper.\n","    Related paper: https://ieeexplore.ieee.org/abstract/document/7801919\n","    \"\"\"\n","    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"conv1\")(x)\n","    x = layers.MaxPool2D(pool_size=2, padding=\"same\", name=\"pool1\")(x)\n","\n","    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"conv2\")(x)\n","    x = layers.MaxPool2D(pool_size=2, padding=\"same\", name=\"pool2\")(x)\n","\n","    x = layers.Conv2D(256, 3, padding=\"same\", use_bias=False, name=\"conv3\")(x)\n","    x = layers.BatchNormalization(name=\"bn3\")(x)\n","    x = layers.Activation(\"relu\", name=\"relu3\")(x)\n","    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\", name=\"conv4\")(x)\n","    x = layers.MaxPool2D(pool_size=2, strides=(2, 1), padding=\"same\", name=\"pool4\")(x)\n","\n","    x = layers.Conv2D(512, 3, padding=\"same\", use_bias=False, name=\"conv5\")(x)\n","    x = layers.BatchNormalization(name=\"bn5\")(x)\n","    x = layers.Activation(\"relu\", name=\"relu5\")(x)\n","    x = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\", name=\"conv6\")(x)\n","    x = layers.MaxPool2D(pool_size=2, strides=(2, 1), padding=\"same\", name=\"pool6\")(x)\n","\n","    x = layers.Conv2D(512, 2, use_bias=False, name=\"conv7\")(x)\n","    x = layers.BatchNormalization(name=\"bn7\")(x)\n","    x = layers.Activation(\"relu\", name=\"relu7\")(x)\n","\n","    x = layers.Reshape((-1, 512), name=\"reshape7\")(x)\n","    return x\n","\n","\n","def build_model(\n","    num_classes,\n","    weight=None,\n","    preprocess=None,\n","    postprocess=None,\n","    img_shape=img_shape1,\n","    model_name=\"crnn\",\n","):\n","    x = img_input = keras.Input(shape=img_shape)\n","    if preprocess is not None:\n","        x = preprocess(x)\n","\n","    x = vgg_style(x)\n","    x = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True), name=\"bi_lstm1\")(x)\n","    x = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True), name=\"bi_lstm2\")(x)\n","    x = layers.Dense(units=num_classes, name=\"logits\")(x)\n","\n","    if postprocess is not None:\n","        x = postprocess(x)\n","\n","    model = keras.Model(inputs=img_input, outputs=x, name=model_name)\n","    if weight is not None:\n","        model.load_weights(weight, by_name=True, skip_mismatch=True)\n","    return model\n"]},{"cell_type":"markdown","metadata":{},"source":["**Assembling stuff**"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.234508Z","iopub.status.busy":"2023-11-26T10:11:44.234071Z","iopub.status.idle":"2023-11-26T10:11:44.248096Z","shell.execute_reply":"2023-11-26T10:11:44.247121Z","shell.execute_reply.started":"2023-11-26T10:11:44.234474Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dataset_builder': {'ignore_case': True,\n","                     'img_shape': (32, 128, 3),\n","                     'max_img_width': 300,\n","                     'table_path': '/kaggle/input/table/example/table.txt'},\n"," 'eval': {'ann_paths': ['/kaggle/input/icdar2013/Challenge2_Test_Task3_Images/Challenge2_Test_Task3_GT.txt'],\n","          'batch_size': 128,\n","          'dataset_builder': {'ignore_case': True,\n","                              'img_shape': (32, 128, 3),\n","                              'max_img_width': 300,\n","                              'table_path': '/kaggle/input/table/example/table.txt'}},\n"," 'train': {'batch_size_per_replica': 128,\n","           'dataset_builder': {'ignore_case': True,\n","                               'img_shape': (32, 128, 3),\n","                               'max_img_width': 300,\n","                               'table_path': '/kaggle/input/table/example/table.txt'},\n","           'epochs': 20,\n","           'lr_schedule': {'alpha': 0.01,\n","                           'decay_steps': 600000,\n","                           'initial_learning_rate': 0.0001},\n","           'tensorboard': {'histogram_freq': 1, 'profile_batch': 0},\n","           'train_ann_paths': ['/kaggle/input/mjsynth-clean-annotation-files/train.txt',\n","                               '/kaggle/input/mjsynth-clean-annotation-files/val.txt'],\n","           'val_ann_paths': ['/kaggle/input/mjsynth-clean-annotation-files/test.txt']}}\n"]}],"source":["# base=\"./mjsynth/mnt/ramdisk/max/90kDICT32px\"\n","# train_path=base+\"/small_train.txt\"\n","# test_path=base+\"/small_test.txt\"\n","# val_path=base+\"/small_val.txt\"\n","\n","train_path=\"/kaggle/input/mjsynth-clean-annotation-files/train.txt\"\n","test_path=\"/kaggle/input/mjsynth-clean-annotation-files/test.txt\"\n","val_path=\"/kaggle/input/mjsynth-clean-annotation-files/val.txt\"\n","\n","table_path=\"/kaggle/input/table/example/table.txt\"\n","# img_shape_config=[32, None, 3]\n","img_shape_config=img_shape1\n","data = {\n","    \"dataset_builder\": {\n","        \"table_path\": table_path,\n","        \"img_shape\": img_shape_config,\n","        \"max_img_width\": 300,\n","        \"ignore_case\": True\n","    },\n","    \"train\": {\n","        \"dataset_builder\": {\n","            \"table_path\": table_path,\n","            \"img_shape\": img_shape_config,\n","            \"max_img_width\": 300,\n","            \"ignore_case\": True\n","        },\n","        \"train_ann_paths\":  [\n","            train_path,\n","            val_path\n","        ],\n","        \"val_ann_paths\": [test_path],\n","        \"batch_size_per_replica\": 128,\n","        \"epochs\": 20,\n","        \"lr_schedule\": {\n","            \"initial_learning_rate\": 0.0001,\n","            \"decay_steps\": 600000,\n","            \"alpha\": 0.01\n","        },\n","        \"tensorboard\": {\n","            \"histogram_freq\": 1,\n","            \"profile_batch\": 0\n","        }\n","    },\n","    \"eval\": {\n","        \"dataset_builder\": {\n","            \"table_path\": table_path,\n","            \"img_shape\": img_shape_config,\n","            \"max_img_width\": 300,\n","            \"ignore_case\": True\n","        },\n","        \"ann_paths\": [\"/kaggle/input/icdar2013/Challenge2_Test_Task3_Images/Challenge2_Test_Task3_GT.txt\"],\n","        \"batch_size\": 128\n","    }\n","}\n","\n","# Printing the Python dictionary\n","import pprint\n","pprint.pprint(data)\n","\n","config=data[\"train\"]"]},{"cell_type":"markdown","metadata":{},"source":["### Train.py"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.249703Z","iopub.status.busy":"2023-11-26T10:11:44.249398Z","iopub.status.idle":"2023-11-26T10:11:44.261654Z","shell.execute_reply":"2023-11-26T10:11:44.260717Z","shell.execute_reply.started":"2023-11-26T10:11:44.249679Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  2\n"]},{"data":{"text/plain":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n"," PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["# make sure if it is = 2\n","import tensorflow as tf\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","gpus = tf.config.list_physical_devices('GPU')\n","gpus"]},{"cell_type":"markdown","metadata":{},"source":["# Do not run this. load the weights instead, the \".h5\" file"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:44.263028Z","iopub.status.busy":"2023-11-26T10:11:44.262726Z","iopub.status.idle":"2023-11-26T10:11:46.241943Z","shell.execute_reply":"2023-11-26T10:11:46.241023Z","shell.execute_reply.started":"2023-11-26T10:11:44.263003Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dictionary has been written to './outputdata.json'.\n","Model: \"crnn\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_6 (InputLayer)        [(None, 32, 128, 3)]      0         \n","                                                                 \n"," conv1 (Conv2D)              (None, 32, 128, 64)       1792      \n","                                                                 \n"," pool1 (MaxPooling2D)        (None, 16, 64, 64)        0         \n","                                                                 \n"," conv2 (Conv2D)              (None, 16, 64, 128)       73856     \n","                                                                 \n"," pool2 (MaxPooling2D)        (None, 8, 32, 128)        0         \n","                                                                 \n"," conv3 (Conv2D)              (None, 8, 32, 256)        294912    \n","                                                                 \n"," bn3 (BatchNormalization)    (None, 8, 32, 256)        1024      \n","                                                                 \n"," relu3 (Activation)          (None, 8, 32, 256)        0         \n","                                                                 \n"," conv4 (Conv2D)              (None, 8, 32, 256)        590080    \n","                                                                 \n"," pool4 (MaxPooling2D)        (None, 4, 32, 256)        0         \n","                                                                 \n"," conv5 (Conv2D)              (None, 4, 32, 512)        1179648   \n","                                                                 \n"," bn5 (BatchNormalization)    (None, 4, 32, 512)        2048      \n","                                                                 \n"," relu5 (Activation)          (None, 4, 32, 512)        0         \n","                                                                 \n"," conv6 (Conv2D)              (None, 4, 32, 512)        2359808   \n","                                                                 \n"," pool6 (MaxPooling2D)        (None, 2, 32, 512)        0         \n","                                                                 \n"," conv7 (Conv2D)              (None, 1, 31, 512)        1048576   \n","                                                                 \n"," bn7 (BatchNormalization)    (None, 1, 31, 512)        2048      \n","                                                                 \n"," relu7 (Activation)          (None, 1, 31, 512)        0         \n","                                                                 \n"," reshape7 (Reshape)          (None, 31, 512)           0         \n","                                                                 \n"," bi_lstm1 (Bidirectional)    (None, 31, 512)           1574912   \n","                                                                 \n"," bi_lstm2 (Bidirectional)    (None, 31, 512)           1574912   \n","                                                                 \n"," logits (Dense)              (None, 31, 38)            19494     \n","                                                                 \n","=================================================================\n","Total params: 8,723,110\n","Trainable params: 8,720,550\n","Non-trainable params: 2,560\n","_________________________________________________________________\n"]}],"source":["import argparse\n","import pprint\n","import shutil\n","from pathlib import Path\n","\n","import tensorflow as tf\n","import yaml\n","from tensorflow import keras\n","\n","# from dataset_factory import DatasetBuilder\n","# from losses import CTCLoss\n","# from metrics import SequenceAccuracy\n","# from models import build_model\n","\n","config=data[\"train\"]\n","\n","output_directory = \"./output\"\n","\n","# Ensure the directory exists; create it if it doesn't\n","if not os.path.exists(output_directory):\n","    os.makedirs(output_directory)\n","import pathlib\n","\n","directory_path = pathlib.Path('./output')\n","\n","# Check if the directory is not empty\n","# if any(directory_path.iterdir()):\n","#     raise ValueError(f\"The directory '{directory_path}' is not empty.\")\n","\n","#\n","# ==============================================================\n","#\n","\n","import os\n","import json\n","# Specify the directory where you want to create the file\n","output_directory = \"./output\"\n","\n","# Ensure the directory exists; create it if it doesn't\n","if not os.path.exists(output_directory):\n","    os.makedirs(output_directory)\n","\n","# File path to save the JSON data\n","file_path = output_directory+\"data.json\"\n","\n","# Write the dictionary to the JSON file\n","with open(file_path, \"w\") as json_file:\n","    json.dump(data, json_file, indent=4)\n","\n","print(f\"Dictionary has been written to '{file_path}'.\")\n","\n","#\n","# ==============================================================\n","#\n","\n","strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n","# strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])\n","# strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n","batch_size = config[\"batch_size_per_replica\"] * strategy.num_replicas_in_sync\n","\n","dataset_builder = DatasetBuilder(**config[\"dataset_builder\"])\n","train_ds = dataset_builder(config[\"train_ann_paths\"], batch_size, True)\n","val_ds = dataset_builder(config[\"val_ann_paths\"], batch_size, False)\n","\n","with strategy.scope():\n","    lr_schedule = keras.optimizers.schedules.CosineDecay(\n","        **config[\"lr_schedule\"]\n","    )\n","    model = build_model(\n","        dataset_builder.num_classes,\n","        weight=config.get(\"weight\"),\n","        img_shape=config[\"dataset_builder\"][\"img_shape\"],\n","    )\n","    model.compile(\n","        optimizer=keras.optimizers.Adam(lr_schedule),\n","        loss=CTCLoss(),\n","        metrics=[SequenceAccuracy()],\n","    )\n","\n","model.summary()\n","import time\n","model_prefix = \"{epoch}_{val_loss:.4f}_{val_sequence_accuracy:.4f}\"\n","model_path = f\"{output_directory}/{model_prefix}.h5\"\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(model_path, save_weights_only=True,save_freq='epoch'),\n","#     keras.callbacks.ModelCheckpoint(model_path, save_weights_only=True,save_freq='epoch'),\n","    keras.callbacks.TensorBoard(\n","        log_dir=f\"{output_directory}/logs\", **config[\"tensorboard\"]\n","    ),\n","]\n"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:46.269510Z","iopub.status.busy":"2023-11-26T10:11:46.269191Z","iopub.status.idle":"2023-11-26T10:11:46.281104Z","shell.execute_reply":"2023-11-26T10:11:46.280019Z","shell.execute_reply.started":"2023-11-26T10:11:46.269478Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7ec87ee61b40>\n"]}],"source":["strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n","print(strategy)"]},{"cell_type":"markdown","metadata":{},"source":["# Loading model and conitnung to train\n","> before we trained for only one epoch, that takes around 4h. If it's possible, you can train again for x epochs (small x). make sure to save the new weights."]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:46.282619Z","iopub.status.busy":"2023-11-26T10:11:46.282333Z","iopub.status.idle":"2023-11-26T10:11:49.312232Z","shell.execute_reply":"2023-11-26T10:11:49.310881Z","shell.execute_reply.started":"2023-11-26T10:11:46.282595Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"crnn\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 32, 128, 3)]      0         \n","                                                                 \n"," conv1 (Conv2D)              (None, 32, 128, 64)       1792      \n","                                                                 \n"," pool1 (MaxPooling2D)        (None, 16, 64, 64)        0         \n","                                                                 \n"," conv2 (Conv2D)              (None, 16, 64, 128)       73856     \n","                                                                 \n"," pool2 (MaxPooling2D)        (None, 8, 32, 128)        0         \n","                                                                 \n"," conv3 (Conv2D)              (None, 8, 32, 256)        294912    \n","                                                                 \n"," bn3 (BatchNormalization)    (None, 8, 32, 256)        1024      \n","                                                                 \n"," relu3 (Activation)          (None, 8, 32, 256)        0         \n","                                                                 \n"," conv4 (Conv2D)              (None, 8, 32, 256)        590080    \n","                                                                 \n"," pool4 (MaxPooling2D)        (None, 4, 32, 256)        0         \n","                                                                 \n"," conv5 (Conv2D)              (None, 4, 32, 512)        1179648   \n","                                                                 \n"," bn5 (BatchNormalization)    (None, 4, 32, 512)        2048      \n","                                                                 \n"," relu5 (Activation)          (None, 4, 32, 512)        0         \n","                                                                 \n"," conv6 (Conv2D)              (None, 4, 32, 512)        2359808   \n","                                                                 \n"," pool6 (MaxPooling2D)        (None, 2, 32, 512)        0         \n","                                                                 \n"," conv7 (Conv2D)              (None, 1, 31, 512)        1048576   \n","                                                                 \n"," bn7 (BatchNormalization)    (None, 1, 31, 512)        2048      \n","                                                                 \n"," relu7 (Activation)          (None, 1, 31, 512)        0         \n","                                                                 \n"," reshape7 (Reshape)          (None, 31, 512)           0         \n","                                                                 \n"," bi_lstm1 (Bidirectional)    (None, 31, 512)           1574912   \n","                                                                 \n"," bi_lstm2 (Bidirectional)    (None, 31, 512)           1574912   \n","                                                                 \n"," logits (Dense)              (None, 31, 38)            19494     \n","                                                                 \n","=================================================================\n","Total params: 8,723,110\n","Trainable params: 8,720,550\n","Non-trainable params: 2,560\n","_________________________________________________________________\n"]}],"source":["# load Model and continue training\n","config=data[\"train\"]\n","# strategy = tf.distribute.MirroredStrategy()\n","# strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n","# strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n","batch_size = config[\"batch_size_per_replica\"] * strategy.num_replicas_in_sync\n","\n","dataset_builder = DatasetBuilder(**config[\"dataset_builder\"])\n","train_ds = dataset_builder(config[\"train_ann_paths\"], batch_size, True)\n","val_ds = dataset_builder(config[\"val_ann_paths\"], batch_size, False)\n","\n","with strategy.scope():\n","    lr_schedule = keras.optimizers.schedules.CosineDecay(\n","        **config[\"lr_schedule\"]\n","    )\n","    model = build_model(\n","        dataset_builder.num_classes,\n","        weight=config.get(\"weight\"),\n","        img_shape=config[\"dataset_builder\"][\"img_shape\"],\n","    )\n","    model.compile(\n","        optimizer=keras.optimizers.Adam(lr_schedule),\n","        loss=CTCLoss(),\n","        metrics=[SequenceAccuracy()],\n","    )\n","\n","# model = tf.keras.models.load_model('./output/20_18.5377_0.0000.h5')\n","\n","model.load_weights('/kaggle/input/d/antoinemnr/training/one_epoch.h5')\n","output_directory = \"./output_2\"\n","model.summary()\n","import time\n","model_prefix = \"{epoch}_{val_loss:.4f}_{val_sequence_accuracy:.4f}\"\n","model_path = f\"{output_directory}/{model_prefix}.h5\"\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(model_path, save_weights_only=True, period=25),\n","    keras.callbacks.TensorBoard(\n","        log_dir=f\"{output_directory}/logs\", **config[\"tensorboard\"]\n","    ),\n","]\n","#model.fit(\n","#     train_ds,\n","#     epochs=100,\n","#    callbacks=callbacks,\n","#     validation_data=val_ds,\n","#)"]},{"cell_type":"code","execution_count":70,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-11-26T10:11:49.316839Z","iopub.status.busy":"2023-11-26T10:11:49.316534Z","iopub.status.idle":"2023-11-26T10:11:49.322885Z","shell.execute_reply":"2023-11-26T10:11:49.321766Z","shell.execute_reply.started":"2023-11-26T10:11:49.316812Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch size per replica: 128\n","Number of replicas in sync: 2\n","Distributed Strategy:  <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7ec87ee61b40>\n"]}],"source":["# to see if the Distributed computation is working\n","print(\"Batch size per replica:\", config[\"batch_size_per_replica\"])\n","print(\"Number of replicas in sync:\", strategy.num_replicas_in_sync)\n","print(\"Distributed Strategy: \", strategy)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:49.324517Z","iopub.status.busy":"2023-11-26T10:11:49.324235Z","iopub.status.idle":"2023-11-26T10:11:49.784200Z","shell.execute_reply":"2023-11-26T10:11:49.783324Z","shell.execute_reply.started":"2023-11-26T10:11:49.324493Z"},"trusted":true},"outputs":[],"source":["callbacks = [\n","    keras.callbacks.TensorBoard(log_dir='./logs', profile_batch='500,520')\n","]\n"]},{"cell_type":"markdown","metadata":{},"source":["**Minor Test**"]},{"cell_type":"markdown","metadata":{},"source":["## Eval:\n"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:49.786255Z","iopub.status.busy":"2023-11-26T10:11:49.785790Z","iopub.status.idle":"2023-11-26T10:11:49.795274Z","shell.execute_reply":"2023-11-26T10:11:49.794444Z","shell.execute_reply.started":"2023-11-26T10:11:49.786216Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dataset_builder': {'table_path': '/kaggle/input/table/example/table.txt', 'img_shape': (32, 128, 3), 'max_img_width': 300, 'ignore_case': True}, 'train': {'dataset_builder': {'table_path': '/kaggle/input/table/example/table.txt', 'img_shape': (32, 128, 3), 'max_img_width': 300, 'ignore_case': True}, 'train_ann_paths': ['/kaggle/input/mjsynth-clean-annotation-files/train.txt', '/kaggle/input/mjsynth-clean-annotation-files/val.txt'], 'val_ann_paths': ['/kaggle/input/mjsynth-clean-annotation-files/test.txt'], 'batch_size_per_replica': 128, 'epochs': 20, 'lr_schedule': {'initial_learning_rate': 0.0001, 'decay_steps': 600000, 'alpha': 0.01}, 'tensorboard': {'histogram_freq': 1, 'profile_batch': 0}}, 'eval': {'dataset_builder': {'table_path': '/kaggle/input/table/example/table.txt', 'img_shape': (32, 128, 3), 'max_img_width': 300, 'ignore_case': True}, 'ann_paths': ['/kaggle/input/icdar2013/Challenge2_Test_Task3_Images/Challenge2_Test_Task3_GT.txt'], 'batch_size': 128}}\n"]}],"source":["train_path=\"/kaggle/input/mjsynth-clean-annotation-files/train.txt\"\n","test_path=\"/kaggle/input/mjsynth-clean-annotation-files/test.txt\"\n","val_path=\"/kaggle/input/mjsynth-clean-annotation-files/val.txt\"\n","\n","table_path=\"/kaggle/input/table/example/table.txt\"\n","# img_shape_config=[32, None, 3]\n","img_shape_config=img_shape1\n","data = {\n","    \"dataset_builder\": {\n","        \"table_path\": table_path,\n","        \"img_shape\": img_shape_config,\n","        \"max_img_width\": 300,\n","        \"ignore_case\": True\n","    },\n","    \"train\": {\n","        \"dataset_builder\": {\n","            \"table_path\": table_path,\n","            \"img_shape\": img_shape_config,\n","            \"max_img_width\": 300,\n","            \"ignore_case\": True\n","        },\n","        \"train_ann_paths\":  [\n","            train_path,\n","            val_path\n","        ],\n","        \"val_ann_paths\": [test_path],\n","        \"batch_size_per_replica\": 128,\n","        \"epochs\": 20,\n","        \"lr_schedule\": {\n","            \"initial_learning_rate\": 0.0001,\n","            \"decay_steps\": 600000,\n","            \"alpha\": 0.01\n","        },\n","        \"tensorboard\": {\n","            \"histogram_freq\": 1,\n","            \"profile_batch\": 0\n","        }\n","    },\n","    \"eval\": {\n","        \"dataset_builder\": {\n","            \"table_path\": table_path,\n","            \"img_shape\": img_shape_config,\n","            \"max_img_width\": 300,\n","            \"ignore_case\": True\n","        },\n","        \"ann_paths\": [\"/kaggle/input/icdar2013/Challenge2_Test_Task3_Images/Challenge2_Test_Task3_GT.txt\"],\n","        \"batch_size\": 128\n","    }\n","}\n","print(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["This **evaluates** using the icdar13 files of images and annotations"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:49.796917Z","iopub.status.busy":"2023-11-26T10:11:49.796607Z","iopub.status.idle":"2023-11-26T10:11:59.664019Z","shell.execute_reply":"2023-11-26T10:11:59.662871Z","shell.execute_reply.started":"2023-11-26T10:11:49.796880Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Image batch shape: (128, 32, 128, 3)\n","Label batch shape: (128, 19)\n","9/9 [==============================] - 7s 233ms/step - loss: 7.1503 - sequence_accuracy: 0.4977 - edit_distance: 0.3351\n"]},{"data":{"text/plain":["[7.150275707244873, 0.49771690368652344, 0.3351057171821594]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["config=data[\"eval\"]\n","dataset_builder = DatasetBuilder(**config[\"dataset_builder\"])\n","ds = dataset_builder(config[\"ann_paths\"], config[\"batch_size\"], False)\n","for images, labels in ds.take(1):\n","    print(\"Image batch shape:\", images.shape)\n","    print(\"Label batch shape:\", labels.shape)\n","model = build_model(\n","     dataset_builder.num_classes,\n","     weight=\"/kaggle/input/d/antoinemnr/training/one_epoch.h5\",\n","     img_shape=config[\"dataset_builder\"][\"img_shape\"],\n"," )\n","model.compile(loss=CTCLoss(), metrics=[SequenceAccuracy(), EditDistance()])\n","model.evaluate(ds)"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:11:59.665828Z","iopub.status.busy":"2023-11-26T10:11:59.665503Z","iopub.status.idle":"2023-11-26T10:12:01.518691Z","shell.execute_reply":"2023-11-26T10:12:01.517667Z","shell.execute_reply.started":"2023-11-26T10:11:59.665800Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 1s/step\n","Predicted text: reimbursing\n","Probability: 0.5314354\n"]}],"source":["import numpy as np\n","from PIL import Image\n","\n","# Load and preprocess the image\n","image_path = '/kaggle/input/table/example/images/2_Reimbursing_64165.jpg'\n","image = Image.open(image_path).convert('RGB')\n","image = image.resize((128, 32))  # Resize to the input size expected by your model\n","image = np.array(image)\n","image = image / 255.0  # Normalize pixel values if required\n","valid_img = np.expand_dims(image, axis=0)  # Add batch dimension\n","\n","# Load the model weights\n","model.load_weights('//kaggle/input/d/antoinemnr/training/one_epoch.h5')\n","# # predict outputs on validation images\n","\n","#prediction = model.predict(valid_img)\n","y_pred = model.predict(valid_img)   \n","\n","\n","# Instantiate the decoder - replace 'your_table_path.txt' with the actual path to your character table\n","decoder = CTCGreedyDecoder(table_path='/kaggle/input/table/example/table.txt')\n","\n","# Decode the predictions\n","decoded_strings, probabilities = decoder.call(y_pred)\n","\n","# Process the output\n","for string, probability in zip(decoded_strings.numpy(), probabilities.numpy()):\n","    print(\"Predicted text:\", string.decode('utf-8'))\n","    print(\"Probability:\", probability)"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:32:29.378174Z","iopub.status.busy":"2023-11-26T10:32:29.377659Z","iopub.status.idle":"2023-11-26T10:32:29.388543Z","shell.execute_reply":"2023-11-26T10:32:29.387597Z","shell.execute_reply.started":"2023-11-26T10:32:29.378138Z"},"trusted":true},"outputs":[],"source":["import albumentations as A\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","blur = A.Blur(blur_limit=5, p=1.0)\n","rotate = A.Rotate(limit=90, interpolation=1, border_mode=4, value=None, mask_value=None, p=1.0)\n","shift_scale_rotaet = A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=45, interpolation=1, border_mode=4, value=None, mask_value=None, p=1.0)\n","optical_distortion = A.OpticalDistortion(distort_limit=1, shift_limit=0.05, interpolation=1, border_mode=4, p=1.0)\n","grid_distortion = A.GridDistortion(num_steps=6, distort_limit=0.5, interpolation=1, border_mode=4, p=1.0)\n","elastic= A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, interpolation=1, border_mode=4, p=1.0)\n","coarse_dropout = A.CoarseDropout(max_holes=16, max_height=32, max_width=32, min_holes=1, min_height=8, min_width=8, fill_value=0, p=1.0)"]},{"cell_type":"code","execution_count":173,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:57:55.916079Z","iopub.status.busy":"2023-11-26T10:57:55.915681Z","iopub.status.idle":"2023-11-26T10:57:55.921944Z","shell.execute_reply":"2023-11-26T10:57:55.920511Z","shell.execute_reply.started":"2023-11-26T10:57:55.916049Z"},"trusted":true},"outputs":[],"source":["def show_img(img, figsize=(8, 8)):\n","    fig, ax = plt.subplots(figsize=figsize)\n","    ax.imshow(img)\n","    ax.set_yticklabels([])\n","    ax.set_xticklabels([])"]},{"cell_type":"code","execution_count":174,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:58:05.282148Z","iopub.status.busy":"2023-11-26T10:58:05.281758Z","iopub.status.idle":"2023-11-26T10:58:05.497478Z","shell.execute_reply":"2023-11-26T10:58:05.496482Z","shell.execute_reply.started":"2023-11-26T10:58:05.282116Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["image = cv2.imread('/kaggle/input/table/example/images/2_Reimbursing_64165.jpg')  # cute dog #1\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","image = cv2.resize(image, (128, 32))\n","show_img(image)"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:52:43.370676Z","iopub.status.busy":"2023-11-26T10:52:43.370259Z","iopub.status.idle":"2023-11-26T10:52:43.396348Z","shell.execute_reply":"2023-11-26T10:52:43.395397Z","shell.execute_reply.started":"2023-11-26T10:52:43.370646Z"},"trusted":true},"outputs":[],"source":["links = pd.read_csv(\"/kaggle/input/iii5vk/links.csv\")\n","links = links.replace(\"'\", \"\", regex=True)\n","links['GroundTruth'] = links['GroundTruth'].str.lower()"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:52:45.198262Z","iopub.status.busy":"2023-11-26T10:52:45.197866Z","iopub.status.idle":"2023-11-26T10:52:45.208556Z","shell.execute_reply":"2023-11-26T10:52:45.207452Z","shell.execute_reply.started":"2023-11-26T10:52:45.198232Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ImgName</th>\n","      <th>GroundTruth</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test/1002_1.png</td>\n","      <td>private</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test/1002_2.png</td>\n","      <td>parking</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test/1009_1.png</td>\n","      <td>salutes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test/100_1.png</td>\n","      <td>dolce</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test/100_2.png</td>\n","      <td>gabbana</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           ImgName GroundTruth\n","0  test/1002_1.png     private\n","1  test/1002_2.png     parking\n","2  test/1009_1.png     salutes\n","3   test/100_1.png       dolce\n","4   test/100_2.png     gabbana"]},"execution_count":148,"metadata":{},"output_type":"execute_result"}],"source":["links.head()"]},{"cell_type":"code","execution_count":157,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:53:55.768543Z","iopub.status.busy":"2023-11-26T10:53:55.768104Z","iopub.status.idle":"2023-11-26T10:53:55.774231Z","shell.execute_reply":"2023-11-26T10:53:55.773225Z","shell.execute_reply.started":"2023-11-26T10:53:55.768508Z"},"trusted":true},"outputs":[],"source":["df_100 = links.sample(100)"]},{"cell_type":"code","execution_count":172,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:57:36.544784Z","iopub.status.busy":"2023-11-26T10:57:36.544414Z","iopub.status.idle":"2023-11-26T10:57:36.551312Z","shell.execute_reply":"2023-11-26T10:57:36.550187Z","shell.execute_reply.started":"2023-11-26T10:57:36.544755Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","import editdistance\n","\n","def calculate_accuracy(y_true, y_pred):\n","    return accuracy_score(y_true, y_pred)\n","\n","def calculate_edit_distance(y_true, y_pred):\n","    total_distance = 0\n","    for true, pred in zip(y_true, y_pred):\n","        total_distance += editdistance.eval(true, pred)\n","    return total_distance / len(y_true)"]},{"cell_type":"markdown","metadata":{},"source":["#No transformation"]},{"cell_type":"code","execution_count":162,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:04.642414Z","iopub.status.busy":"2023-11-26T10:56:04.641989Z","iopub.status.idle":"2023-11-26T10:56:13.921374Z","shell.execute_reply":"2023-11-26T10:56:13.920245Z","shell.execute_reply.started":"2023-11-26T10:56:04.642382Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.43\n","Edit Distance: 1.62\n"]}],"source":["predictions = []\n","ground_truth = []\n","\n","for index, row in df_100.iterrows():\n","    image_path = \"/kaggle/input/iii5vk/test/\" + row['ImgName']\n","    image = cv2.imread(image_path)  # cute dog #1\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = cv2.resize(image, (128, 32))\n","    image = np.array(image)\n","    image = image / 255.0  # Normalize pixel values if required\n","    valid_img = np.expand_dims(image, axis=0)  # Add batch dimension\n","    \n","\n","    y_pred = model.predict(valid_img, verbose=0)   \n","\n","    decoder = CTCGreedyDecoder(table_path='/kaggle/input/table/example/table.txt')\n","\n","    # Decode the predictions\n","    decoded_strings, probabilities = decoder.call(y_pred)\n","\n","    predictions.append(decoded_strings.numpy()[0].decode('utf-8'))\n","    ground_truth.append(row['GroundTruth'])\n","\n","print(f\"Sequence Accuracy: {calculate_accuracy(predictions, ground_truth)}\")\n","print(f\"Edit Distance: {calculate_edit_distance(predictions, ground_truth)}\")"]},{"cell_type":"code","execution_count":163,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:15.508650Z","iopub.status.busy":"2023-11-26T10:56:15.507718Z","iopub.status.idle":"2023-11-26T10:56:15.517001Z","shell.execute_reply":"2023-11-26T10:56:15.515910Z","shell.execute_reply.started":"2023-11-26T10:56:15.508613Z"},"trusted":true},"outputs":[],"source":["def apply_transformation_and_predict(df, transform_func):\n","    predictions = []\n","    ground_truth = []\n","\n","    for index, row in df.iterrows():\n","        image_path = \"/kaggle/input/iii5vk/test/\" + row['ImgName']\n","        image = cv2.imread(image_path)  # cute dog #1\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        image = cv2.resize(image, (128, 32))\n","        \n","        new_image = transform_func(image = image)['image']\n","        image = np.array(new_image)\n","        image = image / 255.0  # Normalize pixel values if required\n","        valid_img = np.expand_dims(image, axis=0)  # Add batch dimension\n","\n","        y_pred = model.predict(valid_img, verbose=0)   \n","\n","        decoder = CTCGreedyDecoder(table_path='/kaggle/input/table/example/table.txt')\n","\n","        # Decode the predictions\n","        decoded_strings, probabilities = decoder.call(y_pred)\n","        \n","        predictions.append(decoded_strings.numpy()[0].decode('utf-8'))\n","        ground_truth.append(row['GroundTruth'])\n","        \n","    print(f\"Sequence Accuracy: {calculate_accuracy(predictions, ground_truth)}\")\n","    print(f\"Edit Distance: {calculate_edit_distance(predictions, ground_truth)}\")"]},{"cell_type":"code","execution_count":164,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:17.917629Z","iopub.status.busy":"2023-11-26T10:56:17.916720Z","iopub.status.idle":"2023-11-26T10:56:26.520515Z","shell.execute_reply":"2023-11-26T10:56:26.519495Z","shell.execute_reply.started":"2023-11-26T10:56:17.917594Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.33\n","Edit Distance: 2.14\n"]}],"source":["apply_transformation_and_predict(df_100, blur)"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:26.522799Z","iopub.status.busy":"2023-11-26T10:56:26.522398Z","iopub.status.idle":"2023-11-26T10:56:35.275425Z","shell.execute_reply":"2023-11-26T10:56:35.274372Z","shell.execute_reply.started":"2023-11-26T10:56:26.522761Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.04\n","Edit Distance: 4.11\n"]}],"source":["apply_transformation_and_predict(df_100, rotate)"]},{"cell_type":"code","execution_count":166,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:35.278171Z","iopub.status.busy":"2023-11-26T10:56:35.277449Z","iopub.status.idle":"2023-11-26T10:56:43.745131Z","shell.execute_reply":"2023-11-26T10:56:43.743883Z","shell.execute_reply.started":"2023-11-26T10:56:35.278131Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.1\n","Edit Distance: 3.2\n"]}],"source":["apply_transformation_and_predict(df_100, shift_scale_rotaet)"]},{"cell_type":"code","execution_count":167,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:56:47.006139Z","iopub.status.busy":"2023-11-26T10:56:47.005638Z","iopub.status.idle":"2023-11-26T10:56:55.722820Z","shell.execute_reply":"2023-11-26T10:56:55.721762Z","shell.execute_reply.started":"2023-11-26T10:56:47.006103Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.18\n","Edit Distance: 2.26\n"]}],"source":["apply_transformation_and_predict(df_100, optical_distortion)"]},{"cell_type":"code","execution_count":169,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:57:04.176231Z","iopub.status.busy":"2023-11-26T10:57:04.175950Z","iopub.status.idle":"2023-11-26T10:57:12.542361Z","shell.execute_reply":"2023-11-26T10:57:12.541388Z","shell.execute_reply.started":"2023-11-26T10:57:04.176206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.12\n","Edit Distance: 2.32\n"]}],"source":["apply_transformation_and_predict(df_100, grid_distortion)"]},{"cell_type":"code","execution_count":170,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:57:12.544551Z","iopub.status.busy":"2023-11-26T10:57:12.544270Z","iopub.status.idle":"2023-11-26T10:57:21.502540Z","shell.execute_reply":"2023-11-26T10:57:21.501527Z","shell.execute_reply.started":"2023-11-26T10:57:12.544524Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.0\n","Edit Distance: 5.09\n"]}],"source":["apply_transformation_and_predict(df_100, elastic)"]},{"cell_type":"code","execution_count":171,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:57:21.504024Z","iopub.status.busy":"2023-11-26T10:57:21.503633Z","iopub.status.idle":"2023-11-26T10:57:30.223446Z","shell.execute_reply":"2023-11-26T10:57:30.222422Z","shell.execute_reply.started":"2023-11-26T10:57:21.503980Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence Accuracy: 0.02\n","Edit Distance: 3.8\n"]}],"source":["apply_transformation_and_predict(df_100, coarse_dropout)"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:32:34.006260Z","iopub.status.busy":"2023-11-26T10:32:34.005260Z","iopub.status.idle":"2023-11-26T10:32:34.221501Z","shell.execute_reply":"2023-11-26T10:32:34.220542Z","shell.execute_reply.started":"2023-11-26T10:32:34.006218Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["image = cv2.imread('/kaggle/input/table/example/images/2_Reimbursing_64165.jpg')  # cute dog #1\n","image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","image = cv2.resize(image, (128, 32))\n","show_img(image)"]},{"cell_type":"markdown","metadata":{},"source":["# Blur"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:33:53.455179Z","iopub.status.busy":"2023-11-26T10:33:53.454245Z","iopub.status.idle":"2023-11-26T10:33:53.567932Z","shell.execute_reply":"2023-11-26T10:33:53.566970Z","shell.execute_reply.started":"2023-11-26T10:33:53.455146Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 29ms/step\n","Predicted text: reneeroins\n","Probability: 1.8614128e-06\n"]}],"source":["new_image = blur(image = image)['image']\n","image = np.array(new_image)\n","image = image / 255.0  # Normalize pixel values if required\n","valid_img = np.expand_dims(image, axis=0)  # Add batch dimension\n","\n","y_pred = model.predict(valid_img)   \n","\n","decoder = CTCGreedyDecoder(table_path='/kaggle/input/table/example/table.txt')\n","\n","# Decode the predictions\n","decoded_strings, probabilities = decoder.call(y_pred)\n","\n","# Process the output\n","for string, probability in zip(decoded_strings.numpy(), probabilities.numpy()):\n","    print(\"Predicted text:\", string.decode('utf-8'))\n","    print(\"Probability:\", probability)"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2023-11-26T10:41:53.620081Z","iopub.status.busy":"2023-11-26T10:41:53.619085Z","iopub.status.idle":"2023-11-26T10:41:53.632942Z","shell.execute_reply":"2023-11-26T10:41:53.631948Z","shell.execute_reply.started":"2023-11-26T10:41:53.620044Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","import editdistance\n","\n","def calculate_accuracy(y_true, y_pred):\n","    return accuracy_score(y_true, y_pred)\n","\n","def calculate_edit_distance(y_true, y_pred):\n","    total_distance = 0\n","    for true, pred in zip(y_true, y_pred):\n","        total_distance += editdistance.eval(true, pred)\n","    return total_distance / len(y_true)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3852677,"sourceId":6677839,"sourceType":"datasetVersion"},{"datasetId":3956382,"sourceId":6887218,"sourceType":"datasetVersion"},{"datasetId":3977070,"sourceId":6926685,"sourceType":"datasetVersion"},{"datasetId":4030579,"sourceId":7010481,"sourceType":"datasetVersion"},{"datasetId":4030592,"sourceId":7010505,"sourceType":"datasetVersion"},{"datasetId":4031111,"sourceId":7011255,"sourceType":"datasetVersion"},{"datasetId":4032025,"sourceId":7012648,"sourceType":"datasetVersion"},{"datasetId":4032072,"sourceId":7012726,"sourceType":"datasetVersion"},{"datasetId":4054425,"sourceId":7045845,"sourceType":"datasetVersion"},{"datasetId":4056530,"sourceId":7049053,"sourceType":"datasetVersion"}],"dockerImageVersionId":30559,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
